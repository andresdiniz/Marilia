import streamlit as st
import logging # Importar a biblioteca de logging

# ConfiguraÃ§Ã£o da pÃ¡gina DEVE SER A PRIMEIRA CHAMADA
st.set_page_config(
    page_title="AnÃ¡lise de Rotas Inteligente",
    layout="wide",
    page_icon="ðŸ“Š"
)

# Restante das importaÃ§Ãµes
import pandas as pd
import numpy as np
from sqlalchemy import create_engine
# Importar matplotlib e seaborn para o heatmap e decomposiÃ§Ã£o
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.tsa.seasonal import seasonal_decompose
import plotly.express as px # Mantido para caso precise em outros lugares (mapa usa go, previsÃ£o usa go)
import plotly.graph_objects as go
from io import BytesIO
import mysql.connector
import pytz
from pmdarima import auto_arima
from sklearn.metrics import mean_absolute_error
import datetime # Importar datetime para manipular dates
import holidays # Importar a biblioteca holidays para feriados
import time # Importar time para manipulaÃ§Ã£o de tempo
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker


# ConfiguraÃ§Ãµes de compatibilidade do numpy (manter se for necessÃ¡rio no seu ambiente)
# Isso pode nÃ£o ser necessÃ¡rio dependendo da versÃ£o do numpy, mas Ã© seguro manter
if np.__version__.startswith('2.'):
    np.float_ = np.float64
    np.int_ = np.int_
    np.bool_ = np.bool_

# Tema personalizado MELHORADO E COERENTE COM FUNDO ESCURO
# Definindo as cores em variÃ¡veis para fÃ¡cil referÃªncia
PRIMARY_COLOR = "#00AFFF"         # Azul mais claro e vibrante
BACKGROUND_COLOR = "#1E1E1E"      # Cinza escuro para o fundo principal
SECONDARY_BACKGROUND_COLOR = "#2D2D2D" # Cinza um pouco mais claro para sidebar/elementos
ACCENT_COLOR = "#FF4B4B"          # Vermelho para destaque/alertas
TEXT_COLOR = "#FFFFFF"            # Branco
HEADER_FONT = 'Segoe UI', 'sans-serif' # Fonte

custom_theme = f"""
<style>
:root {{
    --primary-color: {PRIMARY_COLOR};
    --background-color: {BACKGROUND_COLOR};
    --secondary-background-color: {SECONDARY_BACKGROUND_COLOR};
    --accent-color: {ACCENT_COLOR};
    --text-color: {TEXT_COLOR};
    --header-font: {', '.join(HEADER_FONT)};
}}

html, body, [class*="css"] {{
    font-family: var(--header-font);
    color: var(--text-color);
    background-color: var(--background-color);
}}

h1, h2, h3, h4, h5, h6 {{
    color: var(--primary-color);
    font-weight: 600;
}}

/* Ajustar a cor do texto dentro de expanders para melhor contraste */
.stExpander {{
    background-color: var(--secondary-background-color);
    padding: 10px; /* Adiciona um pouco de padding */
    border-radius: 8px;
    margin-bottom: 15px; /* EspaÃ§o entre expanders */
}}

.stExpander > div > div > p {{
     color: var(--text-color); /* Garante que o texto dentro do expander seja visÃ­vel */
}}
/* Ajustar cor do header do expander */
.stExpander > div > div > .st-emotion-cache-p5msec {{
    color: var(--text-color); /* Garante que o tÃ­tulo do expander seja visÃ­vel */
}}


.stApp {{
    background-color: var(--background-color);
    color: var(--text-color); /* Garante que o texto geral do app use a cor definida */
}}

/* --- Ajustes especÃ­ficos para a sidebar (MAIS AGRESSIVOS) --- */
/* Usando 'background' shorthand e '!important' mais assertivamente */
.stSidebar {{
    background: var(--secondary-background-color) !important; /* ForÃ§a o fundo escuro */
    color: var(--text-color) !important; /* ForÃ§a a cor do texto geral */
    /* Adicionar propriedades para garantir que cubra a Ã¡rea corretamente, se necessÃ¡rio */
    /* height: 100vh !important; */
    /* position: fixed !important; width: 210px !important; top: 0; left: 0; */
}}

.stSidebar .stMarkdown {{
     color: var(--text-color) !important; /* ForÃ§a cor para markdown */
}}

/* ForÃ§ar a cor do texto para elementos de input e labels dentro da sidebar */
.stSidebar label {{
    color: var(--text-color) !important;
}}

.stSidebar div[data-baseweb="select"] > div {{
     background-color: var(--secondary-background-color) !important;
     color: var(--text-color) !important;
     border: 1px solid #555;
}}

.stSidebar input[type="text"],
.stSidebar input[type="date"],
.stSidebar input[type="number"]
{{
    color: var(--text-color) !important;
    background-color: var(--secondary-background-color) !important;
    border: 1px solid #555;
    border-radius: 4px;
    padding: 5px;
}}

.stSidebar .stSlider [data-baseweb="slider"] > div {{
    background-color: var(--primary-color) !important;
}}

.stSidebar .stRadio > label {{
     color: var(--text-color) !important;
}}

/* Garantir que o texto dos botÃµes na sidebar seja visÃ­vel */
.stSidebar button {{
    color: white !important; /* ForÃ§a a cor do texto do botÃ£o para branco */
}}

/* --- Fim ajustes sidebar --- */


.stButton>button {{
    background-color: var(--primary-color);
    color: white;
    border-radius: 8åŽmpx;
    border: none; /* Remover borda padrÃ£o */
    padding: 10px 20px; /* Padding para melhor aparÃªncia */
    cursor: pointer;
}}

.stButton>button:hover {{
    background-color: #0099E6; /* Cor um pouco mais escura no hover */
}}

.stCheckbox>label {{
    color: var(--text-color);
}}

.stSelectbox>label {{
    color: var(--text-color);
}}
/* Melhorar aparÃªncia do selectbox - Regra global */
.stSelectbox > div[data-baseweb="select"] > div {{
     background-color: var(--secondary-background-color);
     color: var(--text-color);
     border: 1px solid #555;
}}


/* Melhorar aparÃªncia do date input - Regra global */
.stDateInput > label {{
    color: var(--text-color);
}}

.stDateInput input {{
    color: var(--text-color);
    background-color: var(--secondary-background-color);
    border: 1px solid #555; /* Borda sutil */
    border-radius: 4px;
    padding: 5px;
}}

/* Melhorar aparÃªncia do slider - Regra global */
.stSlider > label {{
    color: var(--text-color);
}}

.stSlider [data-baseweb="slider"] > div {{
    background-color: var(--primary-color); /* Cor da barra preenchida */
}}


.stSpinner > div > div {{
    color: var(--primary-color); /* Cor do spinner */
}}

/* Estilo para mensagens de aviso */
.stAlert > div {{
    background-color: rgba(255, 255, 0, 0.1); /* Amarelo semi-transparente */
    color: {TEXT_COLOR};
    border-color: yellow;
}}

/* Estilo para mensagens de erro */
.stAlert[kind="error"] > div {{
    background-color: rgba(255, 0, 0, 0.1); /* Vermelho semi-transparente */
    color: {TEXT_COLOR};
    border-color: red;
}}

/* Estilo para mensagens de sucesso */
.stAlert[kind="success"] > div {{
    background-color: rgba(0, 255, 0, 0.1); /* Verde semi-transparente */
    color: {TEXT_COLOR};
    border-color: green;
}}

/* Adiciona hover effect nos botÃµes */
.stButton button:hover {{
    opacity: 0.9;
    transform: scale(1.02);
    transition: all 0.2s ease-in-out;
}}
/* Ajustar o padding da pÃ¡gina principal */
.stApp > header, .stApp > div {{
    padding-top: 1rem;
    padding-bottom: 1rem;
}}

</style>
"""
st.markdown(custom_theme, unsafe_allow_html=True)


# --- FunÃ§Ãµes de Banco de Dados e Carga ---

# Use st.secrets para credenciais de banco de dados
# Para configurar: crie um arquivo .streamlit/secrets.toml na raiz do seu projeto
# Exemplo:
# [mysql]
# host = "185.213.81.52"
# user = "u335174317_wazeportal"
# password = "@Ndre2025." # Mude isso para sua senha real ou use secrets
# database = "u335174317_wazeportal"

@st.cache_resource # ESSENCIAL: Cache a conexÃ£o do banco de dados
def get_db_connection():
    """
Â  Â  Estabelece e retorna uma conexÃ£o com o banco de dados MySQL.
Â  Â  A conexÃ£o Ã© cacheada pelo Streamlit usando @st.cache_resource.
Â  Â  """
    logging.info("Tentando estabelecer conexÃ£o com o banco de dados (via cache_resource).")
    try:
        conn = mysql.connector.connect(
            host=st.secrets["mysql"]["host"],
            user=st.secrets["mysql"]["user"],
            password=st.secrets["mysql"]["password"],
            database=st.secrets["mysql"]["database"]
Â  Â  Â  Â  )
        # O Streamlit fecharÃ¡ a conexÃ£o automaticamente quando o recurso cacheado for invalidado
        logging.info("ConexÃ£o com o banco de dados estabelecida/reutilizada com sucesso.")
        return conn
    except Exception as e:
        logging.exception("Erro ao conectar ao banco de dados:")
        st.error(f"Erro ao conectar ao banco de dados: {e}")
        st.stop()

@st.cache_resource # Cache o engine SQLAlchemy
def get_cached_sqlalchemy_engine():
    """
Â  Â  Cria e retorna um engine SQLAlchemy cacheado.
Â  Â  """
    logging.info("Tentando criar/reutilizar engine SQLAlchemy (via cache_resource).")
    try:
        engine = create_engine(
            f'mysql+mysqlconnector://{st.secrets["mysql"]["user"]}:{st.secrets["mysql"]["password"]}@{st.secrets["mysql"]["host"]}/{st.secrets["mysql"]["database"]}'
Â  Â  Â  Â  )
        logging.info("Engine SQLAlchemy criada/reutilizada com sucesso.")
        return engine
    except Exception as e:
        logging.exception("Erro ao criar engine SQLAlchemy:")
        st.error(f"Erro ao criar engine SQLAlchemy: {e}")
        # Dependendo do uso, talvez nÃ£o precise parar a aplicaÃ§Ã£o aqui, mas logar e retornar None
        return None # Retorna None ou lanÃ§a exceÃ§Ã£o

# @st.cache_data(ttl=600) # Opcional: Cache os resultados da consulta por 10 minutos
def get_data(start_date=None, end_date=None, route_name=None):
Â  Â  """
Â  Â  Busca dados histÃ³ricos de velocidade com tratamento de palavras reservadas e cache.
Â  Â  Usa a conexÃ£o cacheada.
Â  Â  """
Â  Â  # Obter a conexÃ£o cacheada (Streamlit garante que Ã© a mesma instÃ¢ncia em reruns)
Â  Â  mydb = get_db_connection()
Â  Â  mycursor = None # Inicializar cursor como None
Â  Â  try:
Â  Â  Â  Â  # Usar a conexÃ£o cacheada
Â  Â  Â  Â  mycursor = mydb.cursor()
Â  Â  Â  Â  query = """
Â  Â  Â  Â  Â  Â  SELECT
Â  Â  Â  Â  Â  Â  Â  Â  hr.route_id,
Â  Â  Â  Â  Â  Â  Â  Â  r.name AS route_name,
Â  Â  Â  Â  Â  Â  Â  Â  hr.`data` AS data,
Â  Â  Â  Â  Â  Â  Â  Â  hr.velocidade
Â  Â  Â  Â  Â  Â  FROM historic_routes hr
Â  Â  Â  Â  Â  Â  INNER JOIN routes r
Â  Â  Â  Â  Â  Â  Â  Â  ON hr.route_id = r.id
Â  Â  Â  Â  Â  Â  Â  Â  AND r.id_parceiro = 103
Â  Â  Â  Â  Â  Â  WHERE 1=1
Â  Â  Â  Â  """
Â  Â  Â  Â  conditions = []
Â  Â  Â  Â  params = []
Â  Â  Â  Â  if route_name:
Â  Â  Â  Â  Â  Â  conditions.append("r.name = %s")
Â  Â  Â  Â  Â  Â  params.append(route_name)
Â  Â  Â  Â  if start_date:
Â  Â  Â  Â  Â  Â  conditions.append("hr.`data` >= %s")
Â  Â  Â  Â  Â  Â  params.append(start_date)
Â  Â  Â  Â  if end_date:
Â  Â  Â  Â  Â  Â  end_date_dt = datetime.datetime.strptime(end_date, "%Y-%m-%d") + datetime.timedelta(days=1)
Â  Â  Â  Â  Â  Â  end_date_plus_one = end_date_dt.strftime("%Y-%m-%d")
Â  Â  Â  Â  Â  Â  conditions.append("hr.`data` < %s")
Â  Â  Â  Â  Â  Â  params.append(end_date_plus_one)
Â  Â  Â  Â  if conditions:
Â  Â  Â  Â  Â  Â  query += " AND " + " AND ".join(conditions)
Â  Â  Â  Â  query += " ORDER BY hr.`data` ASC"

Â  Â  Â  Â  logging.info(f"Executando query em get_data: {query} com params: {params}")
Â  Â  Â  Â  mycursor.execute(query, params)
Â  Â  Â  Â  results = mycursor.fetchall()

Â  Â  Â  Â  col_names = [i[0] for i in mycursor.description]
Â  Â  Â  Â  df = pd.DataFrame(results, columns=col_names)

Â  Â  Â  Â  if not df.empty:
Â  Â  Â  Â  Â  Â  # ConversÃ£o segura de tipos
Â  Â  Â  Â  Â  Â  df['data'] = pd.to_datetime(df['data'], errors='coerce').dt.tz_localize(None)
Â  Â  Â  Â  Â  Â  df['velocidade'] = pd.to_numeric(df['velocidade'], errors='coerce')
Â  Â  Â  Â  Â  Â  # Remover linhas onde a conversÃ£o falhou
Â  Â  Â  Â  Â  Â  df.dropna(subset=['data', 'velocidade'], inplace=True)


Â  Â  Â  Â  logging.info(f"get_data retornou {len(df)} registros.")
Â  Â  Â  Â  return df, None
Â  Â  except mysql.connector.Error as err:
Â  Â  Â  Â  logging.error(f"Erro MySQL em get_data [{err.errno}]: {err.msg}", exc_info=True)
Â  Â  Â  Â  return pd.DataFrame(), str(err)
Â  Â  except Exception as e:
Â  Â  Â  Â  logging.error(f"Erro geral em get_data: {str(e)}", exc_info=True)
Â  Â  Â  Â  return pd.DataFrame(), str(e)
Â  Â  finally:
Â  Â  Â  Â  # Sempre fechar o cursor
Â  Â  Â  Â  if mycursor:
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  mycursor.close()
Â  Â  Â  Â  Â  Â  Â  Â  logging.info("Cursor fechado em get_data.")
Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  logging.error(f"Erro ao fechar cursor em get_data: {str(e)}")
Â  Â  Â  Â  # NÃƒO feche a conexÃ£o 'mydb' aqui, ela Ã© gerenciada pelo cache_resource


@st.cache_data(ttl=3600) # Cache dos nomes das rotas por 1 hora (3600 segundos)
def get_all_route_names():
Â  Â  """
Â  Â  Busca todos os nomes de rotas distintos no banco de dados e os cacheia.
Â  Â  Usa a conexÃ£o cacheada.
Â  Â  """
Â  Â  mydb = get_db_connection() # ObtÃ©m a conexÃ£o cacheada
Â  Â  mycursor = None # Inicializar cursor como None
Â  Â  try:
Â  Â  Â  Â  mycursor = mydb.cursor()
Â  Â  Â  Â  query = "SELECT DISTINCT name FROM routes WHERE id_parceiro = 103"
Â  Â  Â  Â  logging.info("Executando query em get_all_route_names.")
Â  Â  Â  Â  mycursor.execute(query)
Â  Â  Â  Â  results = mycursor.fetchall()
Â  Â  Â  Â  names = [row[0] for row in results]
Â  Â  Â  Â  logging.info(f"get_all_route_names retornou {len(names)} nomes.")
Â  Â  Â  Â  return names
Â  Â  except Exception as e:
Â  Â  Â  Â  logging.exception("Erro ao obter nomes das rotas:")
Â  Â  Â  Â  st.error(f"Erro ao obter nomes das rotas: {e}")
Â  Â  Â  Â  return []
Â  Â  finally:
Â  Â  Â  Â  if mycursor:
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  mycursor.close()
Â  Â  Â  Â  Â  Â  Â  Â  logging.info("Cursor fechado em get_all_route_names.")
Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  logging.error(f"Erro ao fechar cursor em get_all_route_names: {str(e)}")
Â  Â  Â  Â  # NÃƒO feche a conexÃ£o 'mydb' aqui, ela Ã© gerenciada pelo cache_resource


@st.cache_data(ttl=3600) # Cache das coordenadas por 1 hora (3600 segundos)
def get_route_coordinates(route_id):
Â  Â  """
Â  Â  Busca as coordenadas geogrÃ¡ficas para uma rota especÃ­fica e as cacheia.
Â  Â  Usa a conexÃ£o cacheada.
Â  Â  """
Â  Â  mydb = get_db_connection() # ObtÃ©m a conexÃ£o cacheada
Â  Â  mycursor = None # Inicializar cursor como None
Â  Â  try:
Â  Â  Â  Â  mycursor = mydb.cursor()
Â  Â  Â  Â  query = """
Â  Â  Â  Â  SELECT rl.x, rl.y
Â  Â  Â  Â  FROM route_lines rl
Â  Â  Â  Â  JOIN routes r ON rl.route_id = r.id
Â  Â  Â  Â  WHERE rl.route_id = %s
Â  Â  Â  Â  AND r.id_parceiro = 103
Â  Â  Â  Â  ORDER BY rl.id"""
Â  Â  Â  Â  logging.info(f"Executando query em get_route_coordinates para route_id {route_id}.")
Â  Â  Â  Â  mycursor.execute(query, (route_id,))
Â  Â  Â  Â  results = mycursor.fetchall()
Â  Â  Â  Â  df = pd.DataFrame(results, columns=['longitude', 'latitude'])
Â  Â  Â  Â  logging.info(f"get_route_coordinates retornou {len(df)} registros.")
Â  Â  Â  Â  return df
Â  Â  except Exception as e:
Â  Â  Â  Â  logging.exception(f"Erro ao obter coordenadas para route_id {route_id}:")
Â  Â  Â  Â  st.error(f"Erro ao obter coordenadas: {e}")
Â  Â  Â  Â  return pd.DataFrame()
Â  Â  finally:
Â  Â  Â  Â  if mycursor:
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  mycursor.close()
Â  Â  Â  Â  Â  Â  Â  Â  logging.info("Cursor fechado em get_route_coordinates.")
Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  logging.error(f"Erro ao fechar cursor em get_route_coordinates: {str(e)}")
Â  Â  Â  Â  # NÃƒO feche a conexÃ£o 'mydb' aqui, ela Ã© gerenciada pelo cache_resource

@st.cache_data(ttl=60) # Cache metadados por 60 segundos (pode ajustar)
def get_route_metadata():
Â  Â  """
Â  Â  Busca metadados das rotas ativas com tratamento robusto de erros
Â  Â  Retorna DataFrame com colunas:
Â  Â  [id, name, jam_level, avg_speed, avg_time, historic_speed, historic_time]
Â  Â  Usa a conexÃ£o cacheada.
Â  Â  """
Â  Â  mydb = get_db_connection() # ObtÃ©m a conexÃ£o cacheada
Â  Â  mycursor = None # Inicializar cursor como None
Â  Â  try:
Â  Â  Â  Â  logging.info("Iniciando busca de metadados...")

Â  Â  Â  Â  # Usar a conexÃ£o cacheada. Verificar se estÃ¡ conectada (embora cache_resource deva garantir)
Â  Â  Â  Â  if not mydb.is_connected():
Â  Â  Â  Â  Â  Â  logging.error("ConexÃ£o do cache_resource nÃ£o estÃ¡ conectada.")
Â  Â  Â  Â  Â  Â  # Tente reconectar? Ou deixe o cache_resource invalidar e criar uma nova?
Â  Â  Â  Â  Â  Â  # Para cache_resource, o Streamlit DEVE tentar religar se necessÃ¡rio.
Â  Â  Â  Â  Â  Â  # Se ainda falhar, talvez o problema seja nas credenciais ou no banco.
Â  Â  Â  Â  Â  Â  return pd.DataFrame()


Â  Â  Â  Â  mycursor = mydb.cursor(dictionary=True)

Â  Â  Â  Â  # Query com filtro id_parceiro = 103
Â  Â  Â  Â  query = """
Â  Â  Â  Â  Â  Â  SELECT
Â  Â  Â  Â  Â  Â  Â  Â  id,
Â  Â  Â  Â  Â  Â  Â  Â  name,
Â  Â  Â  Â  Â  Â  Â  Â  jam_level,
Â  Â  Â  Â  Â  Â  Â  Â  avg_speed,
Â  Â  Â  Â  Â  Â  Â  Â  avg_time,
Â  Â  Â  Â  Â  Â  Â  Â  historic_speed,
Â  Â  Â  Â  Â  Â  Â  Â  historic_time
Â  Â  Â  Â  Â  Â  FROM routes
Â  Â  Â  Â  Â  Â  WHERE id_parceiro = 103
Â  Â  Â  Â  """

Â  Â  Â  Â  logging.info("Executando query em get_route_metadata.")
Â  Â  Â  Â  mycursor.execute(query)
Â  Â  Â  Â  results = mycursor.fetchall()

Â  Â  Â  Â  if not results:
Â  Â  Â  Â  Â  Â  logging.warning("Nenhum dado vÃ¡lido encontrado em get_route_metadata.")
Â  Â  Â  Â  Â  Â  return pd.DataFrame()

Â  Â  Â  Â  df = pd.DataFrame(results)

Â  Â  Â  Â  # ConversÃ£o segura de tipos (jÃ¡ presente, mantida)
Â  Â  Â  Â  conversions = {
Â  Â  Â  Â  Â  Â  'avg_speed': 'float32',
Â  Â  Â  Â  Â  Â  'avg_time': 'int32',
Â  Â  Â  Â  Â  Â  'historic_speed': 'float32',
Â  Â  Â  Â  Â  Â  'historic_time': 'int32'
Â  Â  Â  Â  }

Â  Â  Â  Â  for col, dtype in conversions.items():
Â  Â  Â  Â  Â  Â  if col in df.columns: # Verifica se a coluna existe antes de converter
Â  Â  Â  Â  Â  Â  Â  Â  df[col] = pd.to_numeric(df[col], errors='coerce').astype(dtype)
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  logging.warning(f"Coluna '{col}' nÃ£o encontrada no DataFrame de metadados.")


Â  Â  Â  Â  # Remover linhas invÃ¡lidas apÃ³s conversÃ£o (pode ser ajustado dependendo da necessidade)
Â  Â  Â  Â  # df = df.dropna() # Comentado para nÃ£o remover rotas com alguns valores nulos

Â  Â  Â  Â  logging.info(f"get_route_metadata carregou {len(df)} registros.")
Â  Â  Â  Â  return df

Â  Â  except mysql.connector.Error as err:
Â  Â  Â  Â  logging.error(f"Erro MySQL em get_route_metadata: {err}", exc_info=True)
Â  Â  Â  Â  return pd.DataFrame()
Â  Â  except Exception as e:
Â  Â  Â  Â  logging.error(f"Erro geral em get_route_metadata: {e}", exc_info=True)
Â  Â  Â  Â  return pd.DataFrame()
Â  Â  finally:
Â  Â  Â  Â  if mycursor:
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  mycursor.close()
Â  Â  Â  Â  Â  Â  Â  Â  logging.info("Cursor fechado em get_route_metadata.")
Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  logging.error(f"Erro ao fechar cursor em get_route_metadata: {str(e)}")
Â  Â  Â  Â  # NÃƒO feche a conexÃ£o 'mydb' aqui, ela Ã© gerenciada pelo cache_resource


# --- FunÃ§Ãµes de Processamento e AnÃ¡lise (Mantidas como estÃ£o) ---

def clean_data(df):
Â  Â  # ... (cÃ³digo da funÃ§Ã£o clean_data aqui)
Â  Â  """
Â  Â  Limpa, interpola e adiciona features temporais a um DataFrame de velocidade.

Â  Â  Args:
Â  Â  Â  Â  df (pd.DataFrame): DataFrame bruto com colunas 'data' e 'velocidade'.

Â  Â  Returns:
Â  Â  Â  Â  pd.DataFrame: DataFrame limpo com 'day_of_week' e 'hour' adicionados.
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Retorna DataFrame vazio se todas as velocidades forem nulas ou se o input for vazio.
Â  Â  """
Â  Â  if df.empty:
Â  Â  Â  Â  logging.warning("DataFrame vazio fornecido para clean_data.")
Â  Â  Â  Â  return pd.DataFrame()

Â  Â  df = df.copy()

Â  Â  # Adicionar validaÃ§Ã£o de dados: verificar se todas as velocidades estÃ£o nulas
Â  Â  if df['velocidade'].isnull().all():
Â  Â  Â  Â  st.warning("ApÃ³s o carregamento, todas as velocidades estÃ£o nulas. Verifique os dados de origem ou o perÃ­odo selecionado.")
Â  Â  Â  Â  logging.warning("Todas as velocidades sÃ£o nulas apÃ³s o carregamento em clean_data.")
Â  Â  Â  Â  return pd.DataFrame() # Retorna DataFrame vazio se todos os valores sÃ£o nulos

Â  Â  # Assume que o DataFrame jÃ¡ estÃ¡ filtrado pela rota e perÃ­odo
Â  Â  # e que a coluna 'data' jÃ¡ Ã© datetime sem timezone e 'velocidade' Ã© numÃ©rica
Â  Â  df = df.sort_values('data')

Â  Â  # Aplicar clip e interpolaÃ§Ã£o
Â  Â  df['velocidade'] = df['velocidade'].clip(upper=150) # Limita a velocidade a 150 km/h

Â  Â  # InterpolaÃ§Ã£o e preenchimento de NaN
Â  Â  # df['velocidade'] = df['velocidade'].interpolate(method='linear') # Interpola valores ausentes linearmente
Â  Â  # df['velocidade'] = df['velocidade'].ffill() # Preenche valores restantes com o Ãºltimo valor vÃ¡lido
Â  Â  # df['velocidade'] = df['velocidade'].bfill() # Preenche valores restantes com o prÃ³ximo valor vÃ¡lido

Â  Â  # InterpolaÃ§Ã£o com base no tempo apÃ³s garantir que o Ã­ndice seja datetime
Â  Â  # df = df.set_index('data')
Â  Â  # df['velocidade'] = df['velocidade'].interpolate(method='time').ffill().bfill()
Â  Â  # df = df.reset_index()

Â  Â  # InterpolaÃ§Ã£o apenas na coluna velocidade, sem alterar o Ã­ndice para evitar problemas
Â  Â  df['velocidade'] = df['velocidade'].interpolate(method='linear', limit_direction='both').ffill().bfill()
Â  Â  # O ffill e bfill finais sÃ£o importantes para preencher NaNs no inÃ­cio/fim da sÃ©rie apÃ³s a interpolaÃ§Ã£o

Â  Â  # Recalcular dia da semana e hora apÃ³s interpolaÃ§Ã£o/limpeza, se necessÃ¡rio
Â  Â  # Usar locale para nomes dos dias em portuguÃªs (comentado, pois day_name() jÃ¡ retorna em inglÃªs por padrÃ£o)
Â  Â  # import locale
Â  Â  # locale.setlocale(locale.LC_TIME, 'pt_BR.UTF8') # Configurar localidade (pode precisar instalar no ambiente)
Â  Â  df['day_of_week'] = df['data'].dt.day_name() # Retorna em inglÃªs por padrÃ£o, mapearemos para o heatmap
Â  Â  df['hour'] = df['data'].dt.hour

Â  Â  # Remover linhas onde a velocidade AINDA Ã© NaN apÃ³s todo o processo
Â  Â  initial_rows = len(df)
Â  Â  df.dropna(subset=['velocidade'], inplace=True)
Â  Â  if len(df) < initial_rows:
Â  Â  Â  Â  logging.warning(f"Removidas {initial_rows - len(df)} linhas com velocidade NaN apÃ³s interpolaÃ§Ã£o/preenchimento.")

Â  Â  logging.info(f"clean_data retornou DataFrame com {len(df)} registros.")
Â  Â  return df


def seasonal_decomposition_plot(df):
Â  Â  # ... (cÃ³digo da funÃ§Ã£o seasonal_decomposition_plot aqui)
Â  Â  """
Â  Â  Realiza e plota a decomposiÃ§Ã£o sazonal de uma sÃ©rie temporal de velocidade.

Â  Â  Args:
Â  Â  Â  Â  df (pd.DataFrame): DataFrame com dados limpos e coluna 'data'.
Â  Â  """
Â  Â  logging.info("Iniciando seasonal_decomposition_plot.")
Â  Â  if df.empty:
Â  Â  Â  Â  st.info("NÃ£o hÃ¡ dados para realizar a decomposiÃ§Ã£o sazonal.")
Â  Â  Â  Â  logging.warning("DataFrame vazio fornecido para seasonal_decomposition_plot.")
Â  Â  Â  Â  return

Â  Â  # Garantir que 'data' Ã© o Ã­ndice e a sÃ©rie tem frequÃªncia regular
Â  Â  # Usa a coluna 'data' como Ã­ndice e define a frequÃªncia como 3 minutos
Â  Â  try:
Â  Â  Â  Â  # Garante que a coluna 'data' Ã© datetime antes de setar como Ã­ndice
Â  Â  Â  Â  df['data'] = pd.to_datetime(df['data'])
Â  Â  Â  Â  df_ts = df.set_index('data')['velocidade']
Â  Â  Â  Â  df_ts = df_ts.asfreq('3min') # Define a frequÃªncia da sÃ©rie temporal
Â  Â  except Exception as e:
Â  Â  Â  Â  logging.error(f"Erro ao preparar sÃ©rie temporal para decomposiÃ§Ã£o sazonal: {e}", exc_info=True)
Â  Â  Â  Â  st.warning(f"Erro ao preparar dados para decomposiÃ§Ã£o sazonal: {e}")
Â  Â  Â  Â  return


Â  Â  # Interpolar apenas se houver dados suficientes apÃ³s asfreq
Â  Â  # Verifica a proporÃ§Ã£o de NaNs antes de interpolar
Â  Â  nan_ratio = df_ts.isnull().sum() / len(df_ts) if len(df_ts) > 0 else 0
Â  Â  if nan_ratio > 0.3: # Aumentado o limite para 30% - ajuste conforme necessÃ¡rio
Â  Â  Â  Â  st.warning(f"Muitos dados faltantes ({nan_ratio:.1%}) apÃ³s definir frequÃªncia para interpolaÃ§Ã£o e decomposiÃ§Ã£o sazonal confiÃ¡veis.")
Â  Â  Â  Â  logging.warning(f"Alto percentual de NaNs ({nan_ratio:.1%}) apÃ³s asfreq em seasonal_decomposition_plot.")
Â  Â  Â  Â  return

Â  Â  # Interpolar valores ausentes apÃ³s definir a frequÃªncia
Â  Â  df_ts = df_ts.interpolate(method='time')

Â  Â  # Remover NaNs que possam ter ficado no inÃ­cio/fim apÃ³s a interpolaÃ§Ã£o baseada em tempo
Â  Â  df_ts.dropna(inplace=True)

Â  Â  # O perÃ­odo para sazonalidade diÃ¡ria em dados de 3 em 3 minutos Ã© 480 (24 horas * 60 min / 3 min)
Â  Â  period = 480 # Usando o perÃ­odo padrÃ£o para sazonalidade diÃ¡ria a cada 3 minutos

Â  Â  # Precisa de pelo menos 2 ciclos completos de dados vÃ¡lidos para decomposiÃ§Ã£o sazonal
Â  Â  min_data_points = 2 * period
Â  Â  if len(df_ts) < min_data_points:
Â  Â  Â  Â  st.warning(f"Dados insuficientes ({len(df_ts)} pontos vÃ¡lidos) para decomposiÃ§Ã£o sazonal com perÃ­odo de {period}. NecessÃ¡rio pelo menos {int(min_data_points)} pontos de dados vÃ¡lidos apÃ³s tratamento.")
Â  Â  Â  Â  logging.warning(f"Dados insuficientes ({len(df_ts)}) para decomposiÃ§Ã£o sazonal.")
Â  Â  Â  Â  return

Â  Â  try:
Â  Â  Â  Â  logging.info(f"Realizando decomposiÃ§Ã£o sazonal com {len(df_ts)} pontos e perÃ­odo {period}.")
Â  Â  Â  Â  # model='additive' Ã© geralmente adequado para velocidade onde as variaÃ§Ãµes sÃ£o mais constantes
Â  Â  Â  Â  decomposition = seasonal_decompose(df_ts, model='additive', period=period)

Â  Â  Â  Â  fig, ax = plt.subplots(4, 1, figsize=(12, 10)) # Adiciona componente de ResÃ­duo
Â  Â  Â  Â  decomposition.observed.plot(ax=ax[0], title='Observado')
Â  Â  Â  Â  decomposition.trend.plot(ax=ax[1], title='TendÃªncia')
Â  Â  Â  Â  decomposition.seasonal.plot(ax=ax[2], title=f'Sazonalidade (PerÃ­odo {period})')
Â  Â  Â  Â  decomposition.resid.plot(ax=ax[3], title='ResÃ­duo')
Â  Â  Â  Â  plt.tight_layout()

Â  Â  Â  Â  # Configurar cores dos eixos e tÃ­tulos para o tema escuro
Â  Â  Â  Â  for a in ax:
Â  Â  Â  Â  Â  Â  a.tick_params(axis='x', colors=TEXT_COLOR)
Â  Â  Â  Â  Â  Â  a.tick_params(axis='y', colors=TEXT_COLOR)
Â  Â  Â  Â  Â  Â  a.title.set_color(TEXT_COLOR)
Â  Â  Â  Â  Â  Â  a.xaxis.label.set_color(TEXT_COLOR)
Â  Â  Â  Â  Â  Â  a.yaxis.label.set_color(TEXT_COLOR)
Â  Â  Â  Â  Â  Â  # Fundo dos subplots
Â  Â  Â  Â  Â  Â  a.set_facecolor(SECONDARY_BACKGROUND_COLOR)

Â  Â  Â  Â  # Configurar cor de fundo da figura
Â  Â  Â  Â  fig.patch.set_facecolor(SECONDARY_BACKGROUND_COLOR)
Â  Â  Â  Â  fig.suptitle("DecomposiÃ§Ã£o Sazonal da Velocidade", color=TEXT_COLOR, fontsize=16) # TÃ­tulo geral
Â  Â  Â  Â  plt.subplots_adjust(top=0.95) # Ajustar espaÃ§amento para o tÃ­tulo

Â  Â  Â  Â  st.pyplot(fig)
Â  Â  Â  Â  plt.close(fig) # Fecha a figura para liberar memÃ³ria
Â  Â  except Exception as e:
Â  Â  Â  Â  logging.exception("Erro ao realizar decomposiÃ§Ã£o sazonal:") # Log detalhado
Â  Â  Â  Â  st.warning(f"NÃ£o foi possÃ­vel realizar a decomposiÃ§Ã£o sazonal: {e}")
Â  Â  Â  Â  st.info("Verifique se os dados tÃªm uma frequÃªncia regular ou se hÃ¡ dados suficientes.")


def create_holiday_exog(index):
Â  Â  # ... (cÃ³digo da funÃ§Ã£o create_holiday_exog aqui)
Â  Â  """
Â  Â  Cria features exÃ³genas binÃ¡rias ('is_holiday' e 'is_pre_holiday') para um DateTimeIndex.

Â  Â  Args:
Â  Â  Â  Â  index (pd.DateTimeIndex): Ãndice de tempo para o qual gerar as features.

Â  Â  Returns:
Â  Â  Â  Â  pd.DataFrame: DataFrame com as colunas 'is_holiday' e 'is_pre_holiday'.
Â  Â  """
Â  Â  logging.info("Criando features exÃ³genas de feriado.")
Â  Â  if index is None or index.empty:
Â  Â  Â  Â  logging.warning("Ãndice vazio ou None fornecido para create_holiday_exog.")
Â  Â  Â  Â  return pd.DataFrame(columns=['is_holiday', 'is_pre_holiday'], index=index)

Â  Â  try:
Â  Â  Â  Â  # Obter feriados brasileiros para os anos presentes no Ã­ndice
Â  Â  Â  Â  # Pega o ano mÃ­nimo e mÃ¡ximo para garantir todos os feriados relevantes
Â  Â  Â  Â  start_year = index.min().year
Â  Â  Â  Â  end_year = index.max().year
Â  Â  Â  Â  logging.info(f"Buscando feriados para anos de {start_year} a {end_year}.")
Â  Â  Â  Â  br_holidays = holidays.CountryHoliday('BR', years=range(start_year, end_year + 1))

Â  Â  Â  Â  exog_df = pd.DataFrame(index=index)

Â  Â  Â  Â  # is_holiday: Verifica se a data do timestamp atual Ã© um feriado
Â  Â  Â  Â  exog_df['is_holiday'] = index.to_series().apply(lambda date: date.date() in br_holidays).astype(int)

Â  Â  Â  Â  # is_pre_holiday: Verifica se a data EXATAMENTE 24 horas a partir do timestamp atual Ã© um feriado,
Â  Â  Â  Â  # E a data atual NÃƒO Ã© um feriado.
Â  Â  Â  Â  # Isso requer que o Ã­ndice tenha uma frequÃªncia regular definida por asfreq.
Â  Â  Â  Â  if index.freq is None:
Â  Â  Â  Â  Â  Â  logging.warning("Ãndice sem frequÃªncia definida. Verificando vÃ©spera de feriado com offset de 1 dia calendÃ¡rio.")
Â  Â  Â  Â  Â  Â  # Fallback para frequÃªncia irregular - verifica se o prÃ³ximo dia CALENDAR (24h) Ã© um feriado
Â  Â  Â  Â  Â  Â  exog_df['is_pre_holiday'] = index.to_series().apply(
Â  Â  Â  Â  Â  Â  Â  Â  lambda date: (date + pd.Timedelta(days=1)).date() in br_holidays and date.date() not in br_holidays
Â  Â  Â  Â  Â  Â  ).astype(int)
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  logging.info(f"Ãndice com frequÃªncia {index.freq}. Verificando vÃ©spera de feriado com offset de 24 horas exatas.")
Â  Â  Â  Â  Â  Â  # Usa a frequÃªncia para calcular um offset exato de 24 horas
Â  Â  Â  Â  Â  Â  one_day_offset = pd.Timedelta(days=1)
Â  Â  Â  Â  Â  Â  # Cria uma sÃ©rie de dates exatamente 24 horas no futuro com base na frequÃªncia do Ã­ndice
Â  Â  Â  Â  Â  Â  dates_in_24h = index + one_day_offset
Â  Â  Â  Â  Â  Â  # Verifica se a data 24 hours later Ã© um feriado
Â  Â  Â  Â  Â  Â  is_next_day_holiday = dates_in_24h.to_series().apply(lambda date: date.date() in br_holidays).astype(int)
Â  Â  Â  Â  Â  Â  # Uma data Ã© vÃ©spera de feriado se a data 24h later Ã© feriado E a data atual NÃƒO Ã© feriado
Â  Â  Â  Â  Â  Â  exog_df['is_pre_holiday'] = is_next_day_holiday & (exog_df['is_holiday'] == 0)

Â  Â  Â  Â  logging.info("Features exÃ³genas criadas com sucesso.")
Â  Â  Â  Â  return exog_df

# FunÃ§Ã£o de previsÃ£o ARIMA (revisada para usar intervalos de confianÃ§a e tratamento de dados E EXOG)
# NÃ£o cacheamos previsÃµes pois elas dependem de dados recentes e podem ser acionadas pelo usuÃ¡rio
# @st.cache_data # NÃ£o use cache_data para previsÃµes se elas devem ser geradas sob demanda
def create_arima_forecast(df, route_id, steps=10, m_period=480):
Â  Â  # ... (cÃ³digo da funÃ§Ã£o create_arima_forecast aqui)
Â  Â  """
Â  Â  Cria e executa um modelo de previsÃ£o ARIMA sazonal com variÃ¡veis exÃ³genas (feriados/vÃ©speras).

Â  Â  Args:
Â  Â  Â  Â  df (pd.DataFrame): DataFrame com dados histÃ³ricos de velocidade limpos.
Â  Â  Â  Â  route_id (int): ID da rota.
Â  Â  Â  Â  steps (int, optional): NÃºmero de passos futuros para prever. Defaults to 10.
Â  Â  Â  Â  m_period (int, optional): PerÃ­odo sazonal para o auto_arima. Defaults to 480 (diÃ¡rio @ 3min).

Â  Â  Returns:
Â  Â  Â  Â  pd.DataFrame: DataFrame com a previsÃ£o (datas, yhat, limites de confianÃ§a) ou DataFrame vazio em caso de falha.
Â  Â  """
Â  Â  logging.info(f"Iniciando create_arima_forecast para route_id {route_id}.")
Â  Â  if df is None or df.empty:
Â  Â  Â  Â  logging.warning("DataFrame vazio ou None fornecido para create_arima_forecast.")
Â  Â  Â  Â  # Mensagem jÃ¡ exibida na chamada
Â  Â  Â  Â  return pd.DataFrame()

Â  Â  # Preparar dados para auto_arima (jÃ¡ vem limpo)
Â  Â  # Garantir frequÃªncia temporal, interpolando se houver lacunas curtas
Â  Â  try:
Â  Â  Â  Â  # Garante que a coluna 'data' Ã© datetime antes de setar como Ã­ndice
Â  Â  Â  Â  df['data'] = pd.to_datetime(df['data'])
Â  Â  Â  Â  arima_data_full = df.set_index('data')['velocidade'].asfreq('3min').dropna()
Â  Â  except Exception as e:
Â  Â  Â  Â  logging.error(f"Erro ao preparar sÃ©rie temporal para ARIMA: {e}", exc_info=True)
Â  Â  Â  Â  st.warning(f"Erro ao preparar dados para o modelo ARIMA: {e}")
Â  Â  Â  Â  return pd.DataFrame()


Â  Â  # Criar features exÃ³genas (feriados e vÃ©speras) para o perÃ­odo dos dados histÃ³ricos
Â  Â  exog_data_full = create_holiday_exog(arima_data_full.index)

Â  Â  # Alinhar dados da sÃ©rie temporal (y) e dados exÃ³genos (X) usando um join interno
Â  Â  # Isso garante que temos 'y' e 'X' para os mesmos timestamps
Â  Â  try:
Â  Â  Â  Â  combined_df = arima_data_full.to_frame(name='y').join(exog_data_full, how='inner').dropna()
Â  Â  Â  Â  arima_data = combined_df['y']
Â  Â  Â  Â  exog_data = combined_df[['is_holiday', 'is_pre_holiday']]
Â  Â  Â  Â  logging.info(f"Dados alinhados para ARIMA: {len(arima_data)} pontos.")
Â  Â  except Exception as e:
Â  Â  Â  Â  logging.error(f"Erro ao alinhar dados e features exÃ³genas para ARIMA: {e}", exc_info=True)
Â  Â  Â  Â  st.warning(f"Erro ao alinhar dados para o modelo ARIMA: {e}")
Â  Â  Â  Â  return pd.DataFrame()


Â  Â  # Precisa de dados suficientes para o modelo sazonal ARIMA
Â  Â  # Um mÃ­nimo de 2-3 ciclos sazonais Ã© recomendado
Â  Â  min_data_points = 2 * m_period # MÃ­nimo 2 ciclos completos para detectar sazonalidade

Â  Â  if len(arima_data) < min_data_points:
Â  Â  Â  Â  Â st.warning(f"Dados insuficientes ({len(arima_data)} pontos vÃ¡lidos) para treinar um modelo de previsÃ£o ARIMA sazonal robusto com perÃ­odo {m_period}. NecessÃ¡rio pelo menos {int(min_data_points)} pontos vÃ¡lidos apÃ³s alinhamento.")
Â  Â  Â  Â  Â logging.warning(f"Dados insuficientes ({len(arima_data)}) para treinar ARIMA sazonal.")
Â  Â  Â  Â  Â return pd.DataFrame()

Â  Â  try:
Â  Â  Â  Â  # auto_arima encontrarÃ¡ os melhores parÃ¢metros p,d,q,P,D,Q
Â  Â  Â  Â  # Passando o perÃ­odo sazonal 'm' selecionado pelo usuÃ¡rio
Â  Â  Â  Â  # PASSANDO DADOS EXÃ“GENOS (X=exog_data)
Â  Â  Â  Â  with st.spinner(f"Treinando modelo ARIMA para a rota {route_id} com perÃ­odo sazonal m={m_period}..."):
Â  Â  Â  Â  Â  Â  Â logging.info(f"Iniciando treinamento auto_arima com {len(arima_data)} pontos e exog_data.")
Â  Â  Â  Â  Â  Â  Â model = auto_arima(arima_data, X=exog_data, seasonal=True, m=m_period,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  error_action='ignore', suppress_warnings=True,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  stepwise=True, random_state=42,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  n_fits=20) # Limitar o nÃºmero de fits para evitar tempo excessivo
Â  Â  Â  Â  Â  Â  Â logging.info(f"Treinamento auto_arima concluÃ­do. ParÃ¢metros: {model.get_params()}")


Â  Â  Â  Â  # Gerar dates futuras com base na Ãºltima data histÃ³rica e frequÃªncia
Â  Â  Â  Â  last_date = arima_data.index.max()
Â  Â  Â  Â  # A frequÃªncia deve ser compatÃ­vel com m=480 ou 3360 (baseado em 3min)
Â  Â  Â  Â  freq_str = '3min' # Assumindo 3minutos como base

Â  Â  Â  Â  # Cria o range de datas futuras
Â  Â  Â  Â  # start Ã© a prÃ³xima data apÃ³s last_date com a frequÃªncia especificada
Â  Â  Â  Â  future_dates = pd.date_range(start=last_date + pd.Timedelta(freq_str), periods=steps, freq=freq_str)

Â  Â  Â  Â  # Criar features exÃ³genas (feriados e vÃ©speras) para o PERÃODO DA PREVISÃƒO
Â  Â  Â  Â  future_exog_data = create_holiday_exog(future_dates)
Â  Â  Â  Â  # Garantir que o Ã­ndice dos dados exÃ³genos futuros corresponda exatamente Ã s dates futuras
Â  Â  Â  Â  future_exog_data = future_exog_data.reindex(future_dates)

Â  Â  Â  Â  # Verificar se future_exog_data tem as colunas esperadas
Â  Â  Â  Â  if exog_data.columns.tolist() != future_exog_data.columns.tolist():
Â  Â  Â  Â  Â  Â  logging.error(f"Colunas de exÃ³genas futuras nÃ£o correspondem Ã s de treino: {exog_data.columns} vs {future_exog_data.columns}")
Â  Â  Â  Â  Â  Â  st.error("Erro interno: Colunas de variÃ¡veis exÃ³genas nÃ£o correspondem entre treino e previsÃ£o.")
Â  Â  Â  Â  Â  Â  return pd.DataFrame()


Â  Â  Â  Â  # Realizar a previsÃ£o com intervalos de confianÃ§a
Â  Â  Â  Â  # PASSANDO DADOS EXÃ“GENOS FUTUROS (X=future_exog_data)
Â  Â  Â  Â  logging.info(f"Realizando previsÃ£o para {steps} passos com exÃ³genas futuras.")
Â  Â  Â  Â  forecast, conf_int = model.predict(n_periods=steps, return_conf_int=True, X=future_exog_data)


Â  Â  Â  Â  forecast_df = pd.DataFrame({
Â  Â  Â  Â  Â  Â  'ds': future_dates,
Â  Â  Â  Â  Â  Â  'yhat': forecast,
Â  Â  Â  Â  Â  Â  'yhat_lower': conf_int[:, 0], # Limite inferior do intervalo de confianÃ§a
Â  Â  Â  Â  Â  Â  'yhat_upper': conf_int[:, 1], # Limite superior do intervalo de confianÃ§a
Â  Â  Â  Â  Â  Â  'id_route': route_id
Â  Â  Â  Â  })

Â  Â  Â  Â  # Garante que as previsÃµes e intervalos de confianÃ§a nÃ£o sÃ£o negativos
Â  Â  Â  Â  forecast_df[['yhat', 'yhat_lower', 'yhat_upper']] = forecast_df[['yhat', 'yhat_lower', 'yhat_upper']].clip(lower=0)

Â  Â  Â  Â  logging.info(f"PrevisÃ£o ARIMA concluÃ­da para {len(forecast_df)} pontos.")
Â  Â  Â  Â  return forecast_df
Â  Â  except Exception as e:
Â  Â  Â  Â  logging.exception("Erro durante o treinamento ou previsÃ£o do modelo ARIMA:") # Log detalhado
Â  Â  Â  Â  st.error(f"Erro durante o treinamento ou previsÃ£o do modelo ARIMA: {str(e)}")
Â  Â  Â  Â  st.info("Verifique os dados de entrada, a quantidade de dados, ou a configuraÃ§Ã£o do modelo ARIMA.")
Â  Â  Â  Â  return pd.DataFrame()


def save_forecast_to_db(forecast_df):
Â  Â  """
Â  Â  Salva um DataFrame de previsÃ£o no banco de dados usando SQLAlchemy.
Â  Â  Usa o engine cacheado.
Â  Â  """
Â  Â  logging.info("Iniciando save_forecast_to_db.")
Â  Â  if forecast_df is None or forecast_df.empty:
Â  Â  Â  Â  st.warning("NÃ£o hÃ¡ previsÃ£o para salvar no banco de dados.")
Â  Â  Â  Â  logging.warning("DataFrame de previsÃ£o vazio ou None fornecido para save_forecast_to_db.")
Â  Â  Â  Â  return # NÃ£o salva se o DataFrame estiver vazio

Â  Â  # Ajustar nomes de colunas para corresponder Ã  tabela forecast_history
Â  Â  # Assumindo que a tabela forecast_history tem colunas como 'data', 'previsao', 'limite_inferior', 'limite_superior', 'id_rota'
Â  Â  forecast_df_mapped = forecast_df.rename(columns={
Â  Â  Â  Â  'ds': 'data',
Â  Â  Â  Â  'yhat': 'previsao',
Â  Â  Â  Â  'yhat_lower': 'limite_inferior',
Â  Â  Â  Â  'yhat_upper': 'limite_superior',
Â  Â  Â  Â  'id_route': 'id_rota'
Â  Â  })

Â  Â  # Selecionar apenas as colunas que vocÃª quer salvar
Â  Â  cols_to_save = ['data', 'previsao', 'limite_inferior', 'limite_superior', 'id_rota']
Â  Â  # Verificar se todas as colunas existem no DataFrame antes de selecionar
Â  Â  if not all(col in forecast_df_mapped.columns for col in cols_to_save):
Â  Â  Â  Â  missing_cols = [col for col in cols_to_save if col not in forecast_df_mapped.columns]
Â  Â  Â  Â  logging.error(f"Colunas faltando no DataFrame para salvar: {missing_cols}")
Â  Â  Â  Â  st.error(f"Erro ao salvar previsÃ£o: Colunas necessÃ¡rias faltando: {', '.join(missing_cols)}")
Â  Â  Â  Â  return

Â  Â  forecast_df_mapped = forecast_df_mapped[cols_to_save]

Â  Â  # Obter o engine SQLAlchemy cacheado
Â  Â  engine = get_cached_sqlalchemy_engine()
Â  Â  if engine is None:
Â  Â  Â  Â  logging.error("NÃ£o foi possÃ­vel obter o engine SQLAlchemy cacheado.")
Â  Â  Â  Â  st.error("Erro interno: NÃ£o foi possÃ­vel conectar ao banco para salvar a previsÃ£o.")
Â  Â  Â  Â  return

Â  Â  try:
Â  Â  Â  Â  # Usando o gerenciador de contexto do SQLAlchemy para garantir commit/rollback e fechar a conexÃ£o
Â  Â  Â  Â  # if_exists='append' adiciona novas linhas. Se vocÃª precisar evitar duplicatas,
Â  Â  Â  Â  # pode precisar de uma lÃ³gica de upsert ou verificar antes de inserir.
Â  Â  Â  Â  with engine.begin() as connection:
Â  Â  Â  Â  Â  Â  Â logging.info("Salvando previsÃ£o na tabela forecast_history usando SQLAlchemy...")
Â  Â  Â  Â  Â  Â  Â # Converte datetime para tipo compatÃ­vel com SQL, como string ou timestamp
Â  Â  Â  Â  Â  Â  Â forecast_df_mapped['data'] = forecast_df_mapped['data'].dt.strftime('%Y-%m-%d %H:%M:%S')
Â  Â  Â  Â  Â  Â  Â forecast_df_mapped.to_sql('forecast_history', con=connection, if_exists='append', index=False)
Â  Â  Â  Â  Â  Â  Â logging.info("PrevisÃ£o salva com sucesso.")
Â  Â  Â  Â  Â  Â  Â st.toast("PrevisÃ£o salva no banco de dados!", icon="âœ…") # Feedback ao usuÃ¡rio com toast
Â  Â  except Exception as e:
Â  Â  Â  Â  logging.exception("Erro ao salvar previsÃ£o no banco de dados usando SQLAlchemy:") # Log detalhado
Â  Â  Â  Â  st.error(f"Erro ao salvar previsÃ£o no banco de dados: {e}")


def gerar_insights(df):
Â  Â  # ... (cÃ³digo da funÃ§Ã£o gerar_insights aqui)
Â  Â  """
Â  Â  Gera insights automÃ¡ticos sobre a velocidade mÃ©dia, dia mais lento, etc.

Â  Â  Args:
Â  Â  Â  Â  df (pd.DataFrame): DataFrame com dados histÃ³ricos de velocidade processados.

Â  Â  Returns:
Â  Â  Â  Â  str: String formatada com os insights.
Â  Â  """
Â  Â  logging.info("Iniciando gerar_insights.")
Â  Â  insights = []
Â  Â  if df is None or df.empty or 'velocidade' not in df.columns or df['velocidade'].isnull().all():
Â  Â  Â  Â  logging.warning("Dados insuficientes ou invÃ¡lidos para gerar insights.")
Â  Â  Â  Â  return "NÃ£o hÃ¡ dados vÃ¡lidos para gerar insights neste perÃ­odo ou rota."

Â  Â  try:
Â  Â  Â  Â  # Filtrar apenas velocidades vÃ¡lidas para cÃ¡lculos
Â  Â  Â  Â  df_valid_speed = df.dropna(subset=['velocidade'])

Â  Â  Â  Â  if df_valid_speed.empty:
Â  Â  Â  Â  Â  Â  logging.warning("DataFrame vazio apÃ³s remover velocidades NaN para gerar insights.")
Â  Â  Â  Â  Â  Â  return "NÃ£o hÃ¡ dados de velocidade vÃ¡lidos para gerar insights."

Â  Â  Â  Â  media_geral = df_valid_speed['velocidade'].mean()
Â  Â  Â  Â  insights.append(f"ðŸ“Œ Velocidade mÃ©dia geral: **{media_geral:.2f} km/h**")

Â  Â  Â  Â  # Encontrar o dia (data especÃ­fica) com a menor velocidade mÃ©dia dentro do perÃ­odo selecionado
Â  Â  Â  Â  if 'data' in df_valid_speed.columns and not df_valid_speed['data'].empty:
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  # Garante que a coluna 'data' Ã© datetime antes de agrupar
Â  Â  Â  Â  Â  Â  Â  Â  df_valid_speed['data'] = pd.to_datetime(df_valid_speed['data'])
Â  Â  Â  Â  Â  Â  Â  Â  # Agrupar por data (apenas a parte da data)
Â  Â  Â  Â  Â  Â  Â  Â  daily_avg = df_valid_speed.groupby(df_valid_speed['data'].dt.date)['velocidade'].mean()
Â  Â  Â  Â  Â  Â  Â  Â  if not daily_avg.empty:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  dia_mais_lento_date = daily_avg.idxmin()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  velocidade_dia_mais_lento = daily_avg.min()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  insights.append(f"ðŸ“… Dia com a menor velocidade mÃ©dia: **{dia_mais_lento_date.strftime('%d/%m/%Y')}** ({velocidade_dia_mais_lento:.2f} km/h)")
Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  insights.append("NÃ£o foi possÃ­vel calcular a velocidade mÃ©dia diÃ¡ria.")
Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  logging.error(f"Erro ao calcular insight diÃ¡rio: {e}", exc_info=True)
Â  Â  Â  Â  Â  Â  Â  Â  insights.append("Erro ao calcular insight de dia mais lento.")
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  insights.append("Coluna 'data' nÃ£o encontrada ou vazia no DataFrame para insights diÃ¡rios.")


Â  Â  Â  Â  # Encontrar o dia da semana mais lento em mÃ©dia
Â  Â  Â  Â  if 'day_of_week' in df_valid_speed.columns and not df_valid_speed['day_of_week'].empty:
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  weekday_avg = df_valid_speed.groupby('day_of_week')['velocidade'].mean()
Â  Â  Â  Â  Â  Â  Â  Â  if not weekday_avg.empty:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Mapeamento para portuguÃªs e ordenaÃ§Ã£o
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  dias_pt_map = {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'Monday': 'Segunda-feira', 'Tuesday': 'TerÃ§a-feira', 'Wednesday': 'Quarta-feira',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'Thursday': 'Quinta-feira', 'Friday': 'Sexta-feira', 'Saturday': 'SÃ¡bado', 'Sunday': 'Domingo'
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Aplicar o mapeamento e reindexar para ordenar
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  weekday_avg_pt = weekday_avg.rename(index=dias_pt_map).reindex(dias_pt_map.values())

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Encontrar o dia da semana com a menor mÃ©dia (excluindo NaNs que podem surgir do reindex se nÃ£o houver dados para um dia)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  dia_da_semana_mais_lento = weekday_avg_pt.dropna().idxmin() if not weekday_avg_pt.dropna().empty else "N/D"

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  insights.append(f"ðŸ—“ï¸ Dia da semana mais lento (em mÃ©dia): **{dia_da_semana_mais_lento}**")
Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  insights.append("NÃ£o foi possÃ­vel calcular a velocidade mÃ©dia por dia da semana.")
Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  logging.error(f"Erro ao calcular insight por dia da semana: {e}", exc_info=True)
Â  Â  Â  Â  Â  Â  Â  Â  insights.append("Erro ao calcular insight de dia da semana mais lento.")
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  insights.append("Coluna 'day_of_week' nÃ£o encontrada ou vazia no DataFrame para insights por dia da semana.")

Â  Â  Â  Â  # Encontrar a hora do dia mais lenta em mÃ©dia
Â  Â  Â  Â  if 'hour' in df_valid_speed.columns and not df_valid_speed['hour'].empty:
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  hourly_avg = df_valid_speed.groupby('hour')['velocidade'].mean()
Â  Â  Â  Â  Â  Â  Â  Â  if not hourly_avg.empty:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  hora_mais_lenta = hourly_avg.idxmin()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  insights.append(f"ðŸ•’ Hora do dia mais lenta (em mÃ©dia): **{hora_mais_lenta:02d}:00**")
Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  insights.append("NÃ£o foi possÃ­vel calcular a velocidade mÃ©dia por hora do dia.")
Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  logging.error(f"Erro ao calcular insight por hora: {e}", exc_info=True)
Â  Â  Â  Â  Â  Â  Â  Â  insights.append("Erro ao calcular insight de hora mais lenta.")
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  insights.append("Coluna 'hour' nÃ£o encontrada ou vazia no DataFrame para insights por hora.")

Â  Â  Â  Â  logging.info("GeraÃ§Ã£o de insights concluÃ­da.")
Â  Â  Â  Â  return "\n\n".join(insights)

def analyze_current_vs_historical(metadata_df):
Â  Â  # ... (cÃ³digo da funÃ§Ã£o analyze_current_vs_historical aqui)
Â  Â  """
Â  Â  Analisa dados atuais vs histÃ³ricos com tratamento de erros numÃ©ricos
Â  Â  """
Â  Â  try:
Â  Â  Â  Â  logging.info("Iniciando analyze_current_vs_historical")

Â  Â  Â  Â  if not isinstance(metadata_df, pd.DataFrame) or metadata_df.empty:
Â  Â  Â  Â  Â  Â  logging.warning("DataFrame de metadados invÃ¡lido ou vazio para analyze_current_vs_historical.")
Â  Â  Â  Â  Â  Â  return pd.DataFrame()

Â  Â  Â  Â  df = metadata_df.copy()

Â  Â  Â  Â  # Validar a existÃªncia das colunas necessÃ¡rias antes de usÃ¡-las
Â  Â  Â  Â  required_cols = ['avg_time', 'historic_time', 'avg_speed', 'historic_speed']
Â  Â  Â  Â  if not all(col in df.columns for col in required_cols):
Â  Â  Â  Â  Â  Â  missing_cols = [col for col in required_cols if col not in df.columns]
Â  Â  Â  Â  Â  Â  logging.error(f"Colunas necessÃ¡rias faltando em analyze_current_vs_historical: {missing_cols}")
Â  Â  Â  Â  Â  Â  st.error(f"Erro ao analisar dados: Colunas necessÃ¡rias faltando: {', '.join(missing_cols)}")
Â  Â  Â  Â  Â  Â  return pd.DataFrame()

Â  Â  Â  Â  # Substituir zeros para evitar divisÃ£o por zero e converter para float para os cÃ¡lculos
Â  Â  Â  Â  # Usar um pequeno epsilon em vez de 1 pode ser mais seguro se 0 for um valor legÃ­timo mas raro.
Â  Â  Â  Â  # No entanto, para tempo/velocidade histÃ³rica, 0 pode indicar dados ausentes ou invÃ¡lidos.
Â  Â  Â  Â  # Se 0 significa "sem dados histÃ³ricos", substituir por NaN e lidar com fillna(0) apÃ³s o cÃ¡lculo Ã© melhor.
Â  Â  Â  Â  # Assumindo que 0 em historic_time/speed significa dado ausente ou invÃ¡lido para comparaÃ§Ã£o:
Â  Â  Â  Â  df['historic_time'] = df['historic_time'].replace(0, np.nan)
Â  Â  Â  Â  df['historic_speed'] = df['historic_speed'].replace(0, np.nan)

Â  Â  Â  Â  # Garantir que as colunas sÃ£o numÃ©ricas antes de calcular a variaÃ§Ã£o
Â  Â  Â  Â  df['avg_time'] = pd.to_numeric(df['avg_time'], errors='coerce')
Â  Â  Â  Â  df['historic_time'] = pd.to_numeric(df['historic_time'], errors='coerce')
Â  Â  Â  Â  df['avg_speed'] = pd.to_numeric(df['avg_speed'], errors='coerce')
Â  Â  Â  Â  df['historic_speed'] = pd.to_numeric(df['historic_speed'], errors='coerce')


Â  Â  Â  Â  # CÃ¡lculo seguro das variaÃ§Ãµes, lidando com NaNs resultantes da substituiÃ§Ã£o de 0s
Â  Â  Â  Â  # A divisÃ£o por zero resultarÃ¡ em Inf ou NaN, que o .fillna(0) trata.
Â  Â  Â  Â  df['var_time'] = ((df['avg_time'] - df['historic_time']) /
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â df['historic_time']).replace([np.inf, -np.inf], np.nan).fillna(0) * 100
Â  Â  Â  Â  df['var_speed'] = ((df['avg_speed'] - df['historic_speed']) /
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â df['historic_speed']).replace([np.inf, -np.inf], np.nan).fillna(0) * 100


Â  Â  Â  Â  # ClassificaÃ§Ã£o de status (mantida)
Â  Â  Â  Â  conditions = [
Â  Â  Â  Â  Â  Â  (df['var_time'] > 15) | (df['var_speed'] < -15),
Â  Â  Â  Â  Â  Â  (df['var_time'] > 5) | (df['var_speed'] < -5)
Â  Â  Â  Â  ]
Â  Â  Â  Â  choices = ['CrÃ­tico', 'AtenÃ§Ã£o']
Â  Â  Â  Â  df['status'] = np.select(conditions, choices, default='Normal')

Â  Â  Â  Â  logging.info("AnÃ¡lise de dados histÃ³ricos vs atuais concluÃ­da com sucesso")
Â  Â  Â  Â  return df

Â  Â  except Exception as e:
Â  Â  Â  Â  logging.error(f"Erro na anÃ¡lise de dados histÃ³ricos vs atuais: {e}", exc_info=True)
Â  Â  Â  Â  st.error(f"Erro ao analisar dados histÃ³ricos vs atuais: {e}")
Â  Â  Â  Â  return pd.DataFrame()


# Nova funÃ§Ã£o para detalhes completos das conexÃµes (Usa a conexÃ£o cacheada)
def get_db_connections_details():
Â  Â  """
Â  Â  Lista todas as conexÃµes ativas para o banco de dados configurado.
Â  Â  Usa a conexÃ£o cacheada para a consulta, mas a consulta em si lista *todas* as conexÃµes do banco.
Â  Â  """
Â  Â  logging.info("Iniciando get_db_connections_details.")
Â  Â  try:
Â  Â  Â  Â  conn = get_db_connection() # ObtÃ©m a conexÃ£o cacheada
Â  Â  Â  Â  if not conn or not conn.is_connected():
Â  Â  Â  Â  Â  Â  logging.error("ConexÃ£o nÃ£o disponÃ­vel para listar processos.")
Â  Â  Â  Â  Â  Â  st.warning("NÃ£o foi possÃ­vel obter detalhes das conexÃµes (conexÃ£o indisponÃ­vel).")
Â  Â  Â  Â  Â  Â  return []

Â  Â  Â  Â  with conn.cursor(dictionary=True) as cursor:
Â  Â  Â  Â  Â  Â  logging.info(f"Executando query information_schema.processlist para o banco: {st.secrets['mysql']['database']}")
Â  Â  Â  Â  Â  Â  # Nota: Esta query pode precisar de permissÃµes SHOW PROCESSLIST
Â  Â  Â  Â  Â  Â  cursor.execute("""
Â  Â  Â  Â  Â  Â  Â  Â  SELECT
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  id, user, host, db, command, time, state, info
Â  Â  Â  Â  Â  Â  Â  Â  FROM information_schema.processlist
Â  Â  Â  Â  Â  Â  Â  Â  WHERE db = %s
Â  Â  Â  Â  Â  Â  """, (st.secrets["mysql"]["database"],))
Â  Â  Â  Â  Â  Â  results = cursor.fetchall()
Â  Â  Â  Â  Â  Â  logging.info(f"get_db_connections_details retornou {len(results)} processos.")
Â  Â  Â  Â  Â  Â  return results
Â  Â  except Exception as e:
Â  Â  Â  Â  logging.error(f"Erro ao listar conexÃµes: {e}", exc_info=True)
Â  Â  Â  Â  st.error(f"Erro ao listar detalhes das conexÃµes: {e}")
Â  Â  Â  Â  return []
# --- FunÃ§Ã£o Principal do Aplicativo Streamlit ---

def main():
    """
    FunÃ§Ã£o principal que configura a interface do Streamlit, carrega dados
    e exibe anÃ¡lises e previsÃµes.
    """
    # Verificar se as secrets do banco de dados estÃ£o configuradas
    if "mysql" not in st.secrets or not all(k in st.secrets["mysql"] for k in ("host", "user", "password", "database")):
        st.error("As credenciais do banco de dados nÃ£o foram configuradas corretamente no secrets.toml.")
        st.markdown("Por favor, crie ou atualize o arquivo `.streamlit/secrets.toml` na raiz do seu projeto com as informaÃ§Ãµes de conexÃ£o do MySQL.")
        logging.error("Secrets do banco de dados nÃ£o configuradas.") # Log detalhado
        st.stop() # Parar a execuÃ§Ã£o


    with st.sidebar:
        st.title("â„¹ï¸ Painel de Controle")
        st.markdown("""
            Configure a anÃ¡lise de rotas aqui.

            **Funcionalidades:**
            - Visualize dados histÃ³ricos de velocidade
            - Detecte padrÃµes de trÃ¡fego (heatmap, decomposiÃ§Ã£o)
            - Obtenha insights automÃ¡ticos sobre a rota
            - PrevisÃ£o de velocidade para o futuro prÃ³ximo
            - Compare a anÃ¡lise entre diferentes rotas
        """)

        st.subheader("SeleÃ§Ã£o de Rotas")
        # Carregar nomes das rotas de forma eficiente (cached)
        all_route_names = get_all_route_names()
        if not all_route_names:
             st.warning("NÃ£o foi possÃ­vel carregar os nomes das rotas do banco de dados ou nÃ£o hÃ¡ rotas disponÃ­veis.")
             logging.warning("Nenhum nome de rota encontrado no banco de dados.") # Log detalhado
             st.stop() # Parar se nÃ£o houver rotas

        # Usar Ã­ndice para garantir que o selectbox nÃ£o quebre se o nome da rota mudar ou nÃ£o existir
        # Usar session_state para persistir a seleÃ§Ã£o de rota
        if "main_route_select" not in st.session_state or st.session_state.main_route_select not in all_route_names:
             st.session_state.main_route_select = all_route_names[0]

        try:
            default_main_route_index = all_route_names.index(st.session_state.main_route_select)
        except ValueError:
             default_main_route_index = 0

        route_name = st.selectbox(
            "Rota Principal:",
            all_route_names,
            index=default_main_route_index,
            key="main_route_select_box" # Use um key diferente do session_state key
        )
        # Atualiza o session_state key apÃ³s o selectbox
        st.session_state.main_route_select = route_name


        compare_enabled = st.checkbox("Comparar com outra rota", key="compare_checkbox")
        second_route = None
        if compare_enabled:
            available_for_comparison = [r for r in all_route_names if r != route_name]
            if available_for_comparison:
                 # Usar session_state para persistir a seleÃ§Ã£o da rota secundÃ¡ria
                 if "secondary_route_select" not in st.session_state or st.session_state.secondary_route_select not in available_for_comparison:
                      st.session_state.secondary_route_select = available_for_comparison[0]

                 try:
                     default_secondary_route_index = available_for_comparison.index(st.session_state.secondary_route_select)
                 except ValueError:
                      default_secondary_route_index = 0

                 second_route = st.selectbox(
                     "Rota SecundÃ¡ria:",
                     available_for_comparison,
                     index=default_secondary_route_index,
                     key="secondary_route_select_box" # Use um key diferente do session_state key
                 )
                 # Atualiza o session_state key
                 st.session_state.secondary_route_select = second_route

            else:
                 st.info("NÃ£o hÃ¡ outras rotas disponÃ­veis para comparaÃ§Ã£o.")
                 compare_enabled = False # Desabilita comparaÃ§Ã£o se nÃ£o houver outras rotas


        st.subheader("PerÃ­odo de AnÃ¡lise")
        # Usar um seletor de data por rota para flexibilidade na comparaÃ§Ã£o de perÃ­odos diferentes
        # Usar session_state para persistir as dates
        today = datetime.date.today()
        week_ago = today - datetime.timedelta(days=7)

        col_date1, col_date2 = st.columns(2)
        with col_date1:
             # Initialize session state for date range if not exists
             if f"date_range_{route_name}" not in st.session_state:
                 st.session_state[f"date_range_{route_name}"] = (week_ago, today)

             date_range_main_input = st.date_input(
                 f"PerÃ­odo para '{route_name}'",
                 value=st.session_state[f"date_range_{route_name}"],
                 max_value=today,
                 key=f"date_range_{route_name}_input" # Use um key diferente
             )
             # Update session state
             st.session_state[f"date_range_{route_name}"] = date_range_main_input
             date_range_main = st.session_state[f"date_range_{route_name}"] # Use o valor persistido


        date_range_secondary = None
        if compare_enabled and second_route:
             with col_date2:
                 # Initialize session state for date range if not exists
                 if f"date_range_{second_route}" not in st.session_state:
                      st.session_state[f"date_range_{second_route}"] = (week_ago, today)

                 date_range_secondary_input = st.date_input(
                      f"PerÃ­odo para '{second_route}'",
                      value=st.session_state[f"date_range_{second_route}"],
                      max_value=today,
                      key=f"date_range_{second_route}_input" # Use um key diferente
                 )
                 # Update session state
                 st.session_state[f"date_range_{second_route}"] = date_range_secondary_input
                 date_range_secondary = st.session_state[f"date_range_{second_route}"] # Use o valor persistido


        # Validar as dates
        if date_range_main and date_range_main[0] > date_range_main[1]:
            st.error("Data final da rota principal nÃ£o pode ser anterior Ã  data inicial")
            st.stop()
        if compare_enabled and date_range_secondary and date_range_secondary[0] > date_range_secondary[1]:
             st.error("Data final da rota secundÃ¡ria nÃ£o pode ser anterior Ã  data inicial.")
             st.stop()

        st.subheader("ConfiguraÃ§Ãµes ARIMA")
        # Adicionar seletor para o perÃ­odo sazonal (m)
        m_period_options = {
            "DiÃ¡rio (480 pontos @ 3min)": 480,
            "Semanal (3360 pontos @ 3min)": 3360,
            "Mensal (~14400 pontos @ 3min)": 14400 # Aproximado
        }
        selected_m_key = st.selectbox(
            "PerÃ­odo sazonal (m) para ARIMA:",
            list(m_period_options.keys()),
            index=0, # PadrÃ£o diÃ¡rio
            key="arima_m_select"
        )
        arima_m_period = m_period_options[selected_m_key]

        # Adicionar controle para o nÃºmero de passos da previsÃ£o
        forecast_steps = st.slider(f"Quantos pontos futuros prever ({arima_m_period} / freq 3min)?",
                                   min_value=1, max_value=4 * arima_m_period, # Permite prever atÃ© 4 ciclos
                                   value=arima_m_period // 2, # PadrÃ£o: meio ciclo
                                   step=int(arima_m_period / 10), # Passo razoÃ¡vel (1/10 do ciclo)
                                   key="forecast_steps_slider")
        st.info(f"PrevisÃ£o cobrirÃ¡ aproximadamente {forecast_steps * 3} minutos.")


    st.title("ðŸš€ AnÃ¡lise de Rotas Inteligente")
    st.markdown("Selecione as rotas e o perÃ­odo de anÃ¡lise no painel lateral.")

    routes_info = {}
    routes_to_process = [route_name]
    if compare_enabled and second_route:
        routes_to_process.append(second_route)

    # --- Carregamento e Processamento de Dados ---
    st.header("â³ Processando Dados...")
    processed_dfs = {} # DicionÃ¡rio para armazenar os DataFrames processados

    for route in routes_to_process:
        date_range = date_range_main if route == route_name else date_range_secondary
        if date_range is None: # Caso a comparaÃ§Ã£o esteja habilitada, mas a rota secundÃ¡ria nÃ£o tenha range
             continue

        # Converter objetos date para stringsYYYY-MM-DD para passar para get_data
        start_date_str = date_range[0].strftime('%Y-%m-%d')
        end_date_str = date_range[1].strftime('%Y-%m-%d')

        with st.spinner(f'Carregando e processando dados para {route} de {start_date_str} a {end_date_str}...'):
            # Carregar dados filtrando por nome da rota e perÃ­odo (cached)
            raw_df, error = get_data(
                start_date=start_date_str,
                end_date=end_date_str,
                route_name=route
            )

            if error:
                 st.error(f"Erro ao carregar dados para {route}: {error}")
                 logging.error(f"Erro ao carregar dados para {route}: {error}")
                 routes_info[route] = {'data': pd.DataFrame(), 'id': None, 'error': error}
                 continue # Pula para a prÃ³xima rota se houver erro

            if raw_df.empty:
                st.warning(f"Nenhum dado encontrado para a rota '{route}' no perÃ­odo de {start_date_str} a {end_date_str}. Por favor, ajuste o intervalo de dates.")
                logging.warning(f"Nenhum dado encontrado para a rota '{route}' no perÃ­odo {start_date_str} a {end_date_str}.")
                routes_info[route] = {'data': pd.DataFrame(), 'id': None}
                continue # Pula para a prÃ³xima rota

            # Adicionar indicador de qualidade dos dados (dados ausentes)
            total_records = len(raw_df)
            initial_nulls = raw_df['velocidade'].isnull().sum()
            initial_null_percentage = (initial_nulls / total_records) * 100 if total_records > 0 else 0
            st.metric(f"Dados Ausentes Inicialmente ({route})", f"{initial_null_percentage:.1f}%")

            # Obter o ID da rota (assumindo que hÃ¡ apenas um ID por nome no perÃ­odo selecionado)
            try:
                 route_id = raw_df['route_id'].iloc[0]
            except IndexError:
                 st.error(f"NÃ£o foi possÃ­vel obter o ID da rota para '{route}'. Dados insuficientes.")
                 logging.error(f"NÃ£o foi possÃ­vel obter ID da rota para '{route}'. DataFrame vazio ou sem route_id.")
                 routes_info[route] = {'data': pd.DataFrame(), 'id': None}
                 continue

            # Limpar e processar os dados
            processed_df = clean_data(raw_df)

            if processed_df.empty:
                 # Mensagem de warning jÃ¡ exibida dentro de clean_data se todos os valores forem nulos
                 routes_info[route] = {'data': pd.DataFrame(), 'id': None}
                 continue


            routes_info[route] = {
                'data': processed_df,
                'id': route_id
            }
            processed_dfs[route] = processed_df # Armazena para comparaÃ§Ã£o
        st.toast(f"Dados para {route} carregados e processados ({len(processed_df)} registros).", icon="âœ…") # Feedback com toast


    # --- SeÃ§Ã£o de VisualizaÃ§Ã£o ---

    # Se nÃ£o houver dados carregados para nenhuma rota, parar por aqui
    if not routes_info or all(info['data'].empty for info in routes_info.values()):
         st.info("Selecione as rotas e um perÃ­odo com dados disponÃ­veis no painel lateral para continuar.")
         return # Sai da funÃ§Ã£o main se nÃ£o houver dados


    st.header("ðŸ—ºï¸ VisualizaÃ§Ã£o GeogrÃ¡fica")
    # O mapa Ã© exibido por rota dentro do loop de processamento
    for route in routes_to_process:
         # Verifica se a rota foi carregada com sucesso e tem dados
         if route in routes_info and not routes_info[route]['data'].empty:
             route_id = routes_info[route]['id']
             # O expander deve ser dentro do loop para que cada rota tenha seu mapa
             with st.expander(f"Mapa da Rota: {route}", expanded=True):
                  # Obter coordenadas da rota (cached)
                  route_coords = get_route_coordinates(route_id)

                  if not route_coords.empty:
                      # Calcular bounds para centralizar o mapa
                      min_lat, max_lat = route_coords['latitude'].min(), route_coords['latitude'].max()
                      min_lon, max_lon = route_coords['longitude'].min(), route_coords['longitude'].max()

                      # Adicionar um pequeno buffer
                      lat_buffer = (max_lat - min_lat) * 0.05
                      lon_buffer = (max_lon - min_lon) * 0.05

                      center_lat = (max_lat + min_lat) / 2
                      center_lon = (max_lon + min_lon) / 2

                      # Determinar um zoom inicial razoÃ¡vel baseado nos bounds (heurÃ­stica simples)
                      # Calcula a extensÃ£o longitudinal e ajusta o zoom
                      lon_extent = max_lon - min_lon
                      lat_extent = max_lat - min_lat
                      # FÃ³rmula de zoom aproximada (ajuste conforme necessÃ¡rio)
                      if lon_extent > 0 and lat_extent > 0:
                         zoom_lon = 360 / lon_extent
                         zoom_lat = 180 / lat_extent
                         zoom = min(zoom_lon, zoom_lat) * 0.5 # Ajuste o fator (0.5)
                         zoom = min(max(zoom, 10), 15) # Limita o zoom entre 10 e 15
                      else:
                         zoom = 12 # Zoom padrÃ£o se a rota for muito pequena ou um ponto

                      fig = go.Figure(go.Scattermapbox(
                          mode="lines+markers",
                          lon=route_coords['longitude'],
                          lat=route_coords['latitude'],
                          marker={'size': 8, 'color': ACCENT_COLOR},
                          line=dict(width=4, color=PRIMARY_COLOR),
                          hovertext=[f"Ponto {i+1}" for i in range(len(route_coords))],
                          hoverinfo="text+lat+lon" # Mostra texto customizado + lat/lon no tooltip
                      ))

                      fig.update_layout(
                          mapbox={
                              'style': "carto-darkmatter", # Estilo de mapa que combina com o tema escuro
                              'center': {'lat': center_lat, 'lon': center_lon},
                              'zoom': zoom,
                              # bounds podem ser usados para focar na Ã¡rea
                              'bounds': {'west': min_lon - lon_buffer, 'east': max_lon + lon_buffer,
                                         'south': min_lat - lat_buffer, 'north': max_lat + lat_buffer}
                          },
                          margin={"r":0,"t":0,"l":0,"b":0},
                          height=500, # Altura do mapa
                          plot_bgcolor=SECONDARY_BACKGROUND_COLOR, # Fundo do plot
                          paper_bgcolor=SECONDARY_BACKGROUND_COLOR, # Fundo do papel (figura)
                          font=dict(color=TEXT_COLOR), # Cor da fonte global do grÃ¡fico
                          title=f"Mapa da Rota: {route}" # Adiciona tÃ­tulo ao mapa
                      )
                      st.plotly_chart(fig, use_container_width=True)
                  else:
                      st.warning(f"Nenhuma coordenada geogrÃ¡fica encontrada para a rota '{route}'. NÃ£o Ã© possÃ­vel exibir o mapa.")
         elif route in routes_info and 'error' in routes_info[route]:
              st.warning(f"Mapa nÃ£o disponÃ­vel para '{route}' devido a erro no carregamento de dados.")
         else:
             # Isso pode acontecer se compare_enabled for True mas a segunda rota nÃ£o puder ser carregada
             st.info(f"Dados insuficientes para exibir o mapa da rota '{route}'.")

    st.header("ðŸ“ˆ AnÃ¡lise de Momento: HistÃ³rico vs Atual")
    
    try:
        with st.spinner("Carregando anÃ¡lise comparativa..."):
            # Carregar e validar metadados
            route_metadata = get_route_metadata()
            
            if route_metadata.empty:
                st.error("""
                âš ï¸ Dados nÃ£o encontrados. Verifique:
                1. Rotas marcadas como ativas (is_active = 1)
                2. Dados numÃ©ricos preenchidos
                3. ConexÃ£o com o banco de dados
                """)
                return
            
            # Gerar anÃ¡lise
            analysis_df = analyze_current_vs_historical(route_metadata)
            
            if analysis_df.empty:
                st.error("Falha ao gerar anÃ¡lise dos dados")
                return
            
            # Widgets de visualizaÃ§Ã£o
            with st.expander("ðŸ” Principais MÃ©tricas", expanded=True):
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    crit_count = analysis_df[analysis_df['status'] == 'CrÃ­tico'].shape[0]
                    st.metric("Rotas CrÃ­ticas", f"{crit_count} âš ï¸")
                
                with col2:
                    avg_delay = analysis_df['var_time'].mean()
                    st.metric("Atraso MÃ©dio", f"{avg_delay:.1f}%", 
                             help="VariaÃ§Ã£o mÃ©dia do tempo em relaÃ§Ã£o ao histÃ³rico")
                
                with col3:
                    speed_loss = analysis_df['var_speed'].mean()
                    st.metric("Perda de Velocidade", f"{speed_loss:.1f}%",
                             help="VariaÃ§Ã£o mÃ©dia da velocidade em relaÃ§Ã£o ao histÃ³rico")
            
            with st.expander("ðŸš¨ Top 5 Rotas CrÃ­ticas", expanded=True):
                criticas = analysis_df[analysis_df['status'] == 'CrÃ­tico'].head(5)
                
                if not criticas.empty:
                    for _, row in criticas.iterrows():
                        st.markdown(f"""
                        ### {row['name']}
                        - **Tempo Atual:** {row['avg_time']}s (HistÃ³rico: {row['historic_time']}s)
                        - **Velocidade Atual:** {row['avg_speed']:.1f}km/h (HistÃ³rico: {row['historic_speed']:.1f}km/h)
                        - **VariaÃ§Ãµes:** 
                          ðŸ•’ +{row['var_time']:.1f}% tempo | ðŸš— {row['var_speed']:.1f}% velocidade
                        """)
                        st.progress(max(0, min(row['var_time']/100, 1)), 
                                   text=f"Gravidade: {row['var_time']:.1f}%")
                else:
                    st.success("ðŸŽ‰ Nenhuma rota crÃ­tica identificada")
            
            with st.expander("ðŸ“Š Tabela Detalhada", expanded=False):
                st.dataframe(
                    analysis_df,
                    column_config={
                        "name": "Rota",
                        "status": st.column_config.SelectboxColumn(
                            "Status",
                            options=["CrÃ­tico", "AtenÃ§Ã£o", "Normal"],
                            default="Normal"
                        ),
                        "var_time": st.column_config.ProgressColumn(
                            "VariaÃ§Ã£o Tempo",
                            help="Aumento percentual do tempo de viagem",
                            format="+%.1f%%",
                            min_value=-100,
                            max_value=300
                        ),
                        "var_speed": st.column_config.ProgressColumn(
                            "VariaÃ§Ã£o Velocidade",
                            help="MudanÃ§a percentual na velocidade mÃ©dia",
                            format="%.1f%%",
                            min_value=-100,
                            max_value=100
                        )
                    },
                    hide_index=True,
                    use_container_width=True
                )
    
    except Exception as e:
        st.error("Erro crÃ­tico na anÃ¡lise. Verifique os logs para detalhes.")
        logging.critical(f"Falha na anÃ¡lise principal: {str(e)}", exc_info=True)

        
    st.header("ðŸ“Š VisualizaÃ§Ã£o de Dados HistÃ³ricos")

    # --- ComparaÃ§Ã£o Visual de Dados HistÃ³ricos (GrÃ¡fico de Linha Plotly) ---
    if len(processed_dfs) > 0:
         st.subheader("ComparaÃ§Ã£o de Velocidade HistÃ³rica ao Longo do Tempo")
         fig_historical_comparison = go.Figure()

         colors = [PRIMARY_COLOR, ACCENT_COLOR] # Cores para as rotas

         for i, (r_name, r_df) in enumerate(processed_dfs.items()):
              if not r_df.empty:
                   fig_historical_comparison.add_trace(go.Scatter(
                       x=r_df['data'],
                       y=r_df['velocidade'],
                       mode='lines',
                       name=f'HistÃ³rico: {r_name}',
                       line=dict(color=colors[i % len(colors)], width=2) # Usa cores distintas
                   ))
              else:
                   st.info(f"Dados insuficientes para incluir '{r_name}' no grÃ¡fico de comparaÃ§Ã£o histÃ³rica.")


         if len(fig_historical_comparison.data) > 0: # Exibe apenas se houver pelo menos uma rota
              fig_historical_comparison.update_layout(
                  title='Velocidade HistÃ³rica ao Longo do Tempo',
                  xaxis_title="Data/Hora",
                  yaxis_title="Velocidade (km/h)",
                  hovermode='x unified',
                  plot_bgcolor=SECONDARY_BACKGROUND_COLOR,
                  paper_bgcolor=SECONDARY_BACKGROUND_COLOR,
                  font=dict(color=TEXT_COLOR),
                  title_font_color=TEXT_COLOR,
                  xaxis=dict(tickfont=dict(color=TEXT_COLOR), title_font_color=TEXT_COLOR),
                  yaxis=dict(tickfont=dict(color=TEXT_COLOR), title_font_color=TEXT_COLOR),
                  legend=dict(font=dict(color=TEXT_COLOR))
              )
              st.plotly_chart(fig_historical_comparison, use_container_width=True)
         elif compare_enabled:
              st.info("Dados insuficientes para realizar a comparaÃ§Ã£o histÃ³rica entre as rotas selecionadas.")


    # --- SeÃ§Ã£o de AnÃ¡lise Preditiva ---
    st.header("ðŸ“ˆ AnÃ¡lise Preditiva")
    for route in routes_to_process:
        # Verifica se a rota foi carregada com sucesso e tem dados processados
        if route in routes_info and not routes_info[route]['data'].empty:
            processed_df = routes_info[route]['data']
            route_id = routes_info[route]['id']

            # Expander para cada rota
            with st.expander(f"AnÃ¡lise para {route}", expanded=True):

                st.subheader("ðŸ§  Insights AutomÃ¡ticos")
                st.markdown(gerar_insights(processed_df))

                st.subheader("ðŸ“‰ DecomposiÃ§Ã£o Temporal")
                # Passa o df processado que clean_data retornou
                # Esta funÃ§Ã£o usa Matplotlib, a cor do tema Ã© configurada DENTRO dela.
                seasonal_decomposition_plot(processed_df)


                st.subheader("ðŸ”¥ Heatmap HorÃ¡rio por Dia da Semana")
                if not processed_df.empty:
                    pivot_table = processed_df.pivot_table(
                        index='day_of_week',
                        columns='hour',
                        values='velocidade',
                        aggfunc='mean'
                    )

                    # Reordenar dias da semana (em portuguÃªs)
                    dias_ordenados_eng = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
                    dias_pt = ['Segunda', 'TerÃ§a', 'Quarta', 'Quinta', 'Sexta', 'SÃ¡bado', 'Domingo']
                    dia_mapping = dict(zip(dias_ordenados_eng, dias_pt))

                    # Reindexar a tabela pivotada para garantir a ordem dos dias
                    pivot_table = pivot_table.reindex(dias_ordenados_eng)
                    # Renomear o Ã­ndice para portuguÃªs
                    pivot_table.index = pivot_table.index.map(dia_mapping)


                    # --- Usar Matplotlib/Seaborn Heatmap ---
                    # Criar uma figura e eixos Matplotlib
                    fig_mpl, ax_mpl = plt.subplots(figsize=(12, 8)) # Tamanho da figura

                    # Gerar o heatmap usando Seaborn
                    sns.heatmap(
                        pivot_table,
                        annot=True,      # Mostrar os valores nas cÃ©lulas
                        fmt=".0f",       # Formatar os valores para 0 casas decimais (inteiro) <--- Corrigido para 0 casas decimais
                        cmap="viridis",  # Mapa de cores (similar ao Viridis do Plotly)
                        linewidths=.5,   # Adicionar linhas entre as cÃ©lulas para clareza
                        ax=ax_mpl        # Desenhar no eixo Matplotlib criado
                         # annot_kws={"color": TEXT_COLOR} # Opcional: cor da fonte da anotaÃ§Ã£o (pode prejudicar leitura)
                    )

                    # Configurar tÃ­tulos e labels dos eixos para o tema escuro
                    ax_mpl.set_title('Velocidade MÃ©dia por Dia da Semana e Hora', color=TEXT_COLOR)
                    ax_mpl.set_xlabel('Hora do Dia', color=TEXT_COLOR)
                    ax_mpl.set_ylabel('Dia da Semana', color=TEXT_COLOR)

                    # Configurar cor dos ticks dos eixos e fundo do plot
                    ax_mpl.tick_params(axis='x', colors=TEXT_COLOR)
                    ax_mpl.tick_params(axis='y', colors=TEXT_COLOR)
                    ax_mpl.set_facecolor(SECONDARY_BACKGROUND_COLOR) # Fundo da Ã¡rea do plot

                    # Configurar cor de fundo da figura inteira
                    fig_mpl.patch.set_facecolor(SECONDARY_BACKGROUND_COLOR) # Fundo da figura

                    # Configurar a cor da barra de cor (colorbar)
                    cbar = ax_mpl.collections[0].colorbar # Obter o objeto colorbar
                    if cbar:
                         cbar.ax.tick_params(colors=TEXT_COLOR) # Cor dos ticks
                         cbar.set_label('Velocidade MÃ©dia (km/h)', color=TEXT_COLOR) # Cor do label

                    # Exibir a figura Matplotlib no Streamlit
                    st.pyplot(fig_mpl)
                    plt.close(fig_mpl) # Fechar a figura para liberar memÃ³ria

                else:
                    st.info("Dados insuficientes para gerar o Heatmap.")


                st.subheader("ðŸ”® PrevisÃ£o de Velocidade (ARIMA)")

                # BotÃ£o para rodar a previsÃ£o (usa forecast_steps e arima_m_period definidos na sidebar)
                if st.button(f"Gerar PrevisÃ£o para {route}", key=f"generate_forecast_{route}"):
                     forecast_df = pd.DataFrame() # Initialize DataFrame

                     # --- Try/Except para a GeraÃ§Ã£o da PrevisÃ£o ARIMA ---
                     try:
                         st.info(f"Iniciando geraÃ§Ã£o da previsÃ£o ARIMA para {route} com perÃ­odo sazonal m={arima_m_period} e {forecast_steps} passos futuros...")
                         # Chamada da funÃ§Ã£o de previsÃ£o ARIMA (agora com exÃ³genas e m_period)
                         forecast_df = create_arima_forecast(processed_df, route_id, steps=forecast_steps, m_period=arima_m_period)

                         if not forecast_df.empty:
                             st.success(f"PrevisÃ£o gerada para os prÃ³ximos {forecast_steps * 3} minutos.")
                             st.toast("PrevisÃ£o gerada!", icon="âœ…") # Feedback com toast


                             # --- Try/Except para Plotar o GrÃ¡fico de PrevisÃ£o ---
                             try:
                                 st.info("Gerando grÃ¡fico de previsÃ£o...")
                                 fig_forecast = go.Figure()

                                 # Adiciona os dados histÃ³ricos
                                 fig_forecast.add_trace(go.Scatter(
                                     x=processed_df['data'],
                                     y=processed_df['velocidade'],
                                     mode='lines',
                                     name=f'HistÃ³rico: {route}', # Nome da rota no histÃ³rico
                                     line=dict(color=TEXT_COLOR, width=2) # Cor para o histÃ³rico
                                 ))

                                 # Adiciona a previsÃ£o
                                 fig_forecast.add_trace(go.Scatter(
                                     x=forecast_df['ds'],
                                     y=forecast_df['yhat'],
                                     mode='lines',
                                     name=f'PrevisÃ£o: {route}', # Nome da rota na previsÃ£o
                                     line=dict(color=PRIMARY_COLOR, width=3) # Cor primÃ¡ria para a previsÃ£o
                                 ))

                                 # Adiciona o intervalo de confianÃ§a
                                 fig_forecast.add_trace(go.Scatter(
                                     x=pd.concat([forecast_df['ds'], forecast_df['ds'][::-1]]), # Dates para o polÃ­gono (ida e volta)
                                     y=pd.concat([forecast_df['yhat_upper'], forecast_df['yhat_lower'][::-1]]), # Limites (superior e inferior invertido)
                                     fill='toself', # Preenche a Ã¡rea entre as duas linhas
                                     fillcolor='rgba(0, 175, 255, 0.2)', # Cor semi-transparente (similar ao PRIMARY_COLOR)
                                     line=dict(color='rgba(255,255,255,0)'), # Linha invisÃ­vel
                                     name=f'Intervalo de ConfianÃ§a 95% ({route})'
                                 ))

                                 # Configura o layout do grÃ¡fico de previsÃ£o
                                 fig_forecast.update_layout(
                                     title=f'PrevisÃ£o de Velocidade para {route}',
                                     xaxis_title="Data/Hora",
                                     yaxis_title="Velocidade (km/h)",
                                     hovermode='x unified', # Agrupa tooltips por eixo X
                                     plot_bgcolor=SECONDARY_BACKGROUND_COLOR,
                                     paper_bgcolor=SECONDARY_BACKGROUND_COLOR,
                                     font=dict(color=TEXT_COLOR),
                                     title_font_color=TEXT_COLOR,
                                     xaxis=dict(tickfont=dict(color=TEXT_COLOR), title_font_color=TEXT_COLOR),
                                     yaxis=dict(tickfont=dict(color=TEXT_COLOR), title_font_color=TEXT_COLOR),
                                     legend=dict(font=dict(color=TEXT_COLOR))
                                 )

                                 st.plotly_chart(fig_forecast, use_container_width=True)
                                 st.success("GrÃ¡fico de previsÃ£o gerado.")


                             except Exception as e:
                                 logging.exception("Erro ao gerar ou exibir o grÃ¡fico de previsÃ£o:") # Log detalhado
                                 st.error(f"Erro ao gerar ou exibir o grÃ¡fico de previsÃ£o: {e}")
                                 st.info("Verifique se hÃ¡ dados suficientes na previsÃ£o gerada ou se hÃ¡ problemas na configuraÃ§Ã£o do grÃ¡fico Plotly.")

                             # --- Try/Except para Salvar no Banco de Dados ---
                             # O botÃ£o de salvar sÃ³ aparece APÃ“S a previsÃ£o ser gerada e plotada
                             if st.button(f"Salvar PrevisÃ£o no Banco de Dados para {route}", key=f"save_forecast_{route}"):
                                  save_forecast_to_db(forecast_df)


                         else:
                             st.warning("PrevisÃ£o nÃ£o gerada ou DataFrame de previsÃ£o vazio. NÃ£o Ã© possÃ­vel exibir o grÃ¡fico ou salvar.")
                             st.toast("PrevisÃ£o falhou!", icon="âŒ") # Feedback com toast

                     except Exception as e:
                         logging.exception("Erro fatal durante a geraÃ§Ã£o da previsÃ£o ARIMA:") # Log detalhado
                         st.error(f"Erro fatal durante a geraÃ§Ã£o da previsÃ£o ARIMA: {e}")
                         st.info("Verifique os dados de entrada, a quantidade de dados, ou a configuraÃ§Ã£o do modelo ARIMA.")
                         st.toast("PrevisÃ£o falhou!", icon="âŒ") # Feedback com toast


                # Mensagem inicial antes de gerar a previsÃ£o
                elif f"generate_forecast_{route}" not in st.session_state:
                    st.info("Configure o perÃ­odo sazonal e os passos futuros na sidebar e clique em 'Gerar PrevisÃ£o'.")


        # Adiciona uma linha separadora entre as anÃ¡lises de rotas se houver mais de uma
        if len(routes_to_process) > 1 and routes_to_process.index(route) < len(routes_to_process) - 1:
            st.markdown("---") # Linha horizontal

    # Mensagem final caso nenhuma rota tenha dados
    if not routes_info or all(info['data'].empty for info in routes_info.values()):
         st.info("Nenhuma anÃ¡lise exibida. Selecione rotas com dados disponÃ­veis.")
    
    # Em qualquer seÃ§Ã£o do seu cÃ³digo:
    with st.expander("ðŸ” Detalhes das ConexÃµes Ativas"):
        connections = get_db_connections_details()
        if connections:
            st.dataframe(
                pd.DataFrame(connections),
                column_config={
                    "time": st.column_config.ProgressColumn(
                        "Tempo (s)",
                        format="%ds",
                        min_value=0,
                        max_value=3600
                    )
                },
                hide_index=True
            )
        else:
            st.info("Nenhuma conexÃ£o ativa encontrada")
    
    # Adicione no final da funÃ§Ã£o main()
    logging.info(f"ConexÃµes ativas ao finalizar: {get_db_connections_details()}")


# --- Executa o aplicativo Streamlit ---
if __name__ == "__main__":
    main()